!pip install --target=$my_path MeCab
!pip install unidic-lite
!pip install --target=$my_path unidic-lite
!pip install --target=$my_path mecab-python3

# -*- coding: utf-8 -*-

from google.colab import drive 
drive.mount('/content/gdrive')

# Load the Excel file
df = pd.read_excel(r'/content/gdrive/MyDrive/daegul_project.xlsx', sheet_name='Sheet1')

import MeCab
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA


# Extract the text data from the "Text" column into a list
fall_22 = df['22_fall'].tolist()
spring_22 = df['22_spring'].tolist()
fall_21 = df['21_fall'].tolist()
spring_21 = df['21_spring'].tolist()
fall_20 = df['20_fall'].tolist()

whole_list = []
#whole_list.extend(fall_22)
#whole_list.extend(spring_22)
#whole_list.extend(fall_21)
#whole_list.extend(spring_21)
whole_list.extend(fall_20)

# Remove NaN values from the text data
whole_list = [sentence for sentence in whole_list if isinstance(sentence, str)]

# Define the tokenizer using Mecab
tokenizer = MeCab.Tagger()

def mecab_tokenizer(text):
    """
    Tokenize text using Mecab.
    """
    text = text.strip()
    node = tokenizer.parse(text)
    words = []
    for row in node.split("\n"):
        if "\t" in row:
            word = row.split("\t")[0]
            words.append(word)
    return words

# Create CountVectorizer with Mecab tokenizer
count_vect = CountVectorizer(tokenizer=mecab_tokenizer)

# Fit and transform the text data
X_counts = count_vect.fit_transform(whole_list)

# Print the vocabulary and document-term matrix
X = X_counts.toarray()

# Apply k-means clustering with 5 clusters
kmeans = KMeans(n_clusters=5, random_state=42)
kmeans.fit(X)

# Print out the first five sentences associated with each cluster
for i in range(5):
    print(f'Cluster {i}:')
    cluster_sentences = np.array(whole_list)[kmeans.labels_ == i]
    for sentence in cluster_sentences[:5]:
        print(f'- {sentence}')
    print()

# Apply PCA to reduce the dimensionality of the data to 2 dimensions for visualization
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Determine the optimal number of clusters using the elbow method
#distortions = []
#for i in range(3, 10):
#    kmeans = KMeans(n_clusters=i, random_state=42)
#    kmeans.fit(X)
#    distortions.append(kmeans.inertia_)
    
#plt.plot(range(3, 10), distortions, marker='o')
#plt.xlabel('Number of clusters')
#plt.ylabel('Distortion')
#plt.show()

# Visualize the clusters using a scatter plot
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_, cmap='rainbow')
plt.xlabel('PC1 ({:.2f}%)'.format(pca.explained_variance_ratio_[0]*100))
plt.ylabel('PC2 ({:.2f}%)'.format(pca.explained_variance_ratio_[1]*100))
# plt.title('추천수 150 이상의 베스트 게시판의 제목들을 군집분석한 그래프')

plt.show()
